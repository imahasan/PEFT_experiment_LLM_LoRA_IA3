Running from this directory: /cluster/home/mdaha/final_fine-tuning-for-logs
Name of job: lora_ft_1gpu_3epo_final
ID of job: 21080493
The job was run on these nodes: idun-01-03
ðŸš€ Using device: cuda
âœ… Log file creation test passed.
ðŸ“‚ Loading Dataset...
ðŸ’¾ Loading model with Accelerate...
ðŸ”§ Configuring LoRA...
Weight Decay: 0.01
Batch Size: 1
Total training samples: 6961
{'loss': 234.9228, 'grad_norm': 14.045204162597656, 'learning_rate': 1.9846390168970814e-05, 'epoch': 0.02}
{'loss': 236.105, 'grad_norm': 16.822696685791016, 'learning_rate': 1.969278033794163e-05, 'epoch': 0.05}
{'loss': 232.7266, 'grad_norm': 20.386430740356445, 'learning_rate': 1.9539170506912443e-05, 'epoch': 0.07}
{'loss': 233.5305, 'grad_norm': 23.32961082458496, 'learning_rate': 1.9385560675883256e-05, 'epoch': 0.09}
{'loss': 232.0686, 'grad_norm': 27.278255462646484, 'learning_rate': 1.923195084485407e-05, 'epoch': 0.11}
{'loss': 230.4166, 'grad_norm': 30.332515716552734, 'learning_rate': 1.9078341013824884e-05, 'epoch': 0.14}
{'loss': 226.601, 'grad_norm': 28.39542007446289, 'learning_rate': 1.89247311827957e-05, 'epoch': 0.16}
{'loss': 226.8233, 'grad_norm': 31.24574851989746, 'learning_rate': 1.8771121351766516e-05, 'epoch': 0.18}
{'loss': 225.1999, 'grad_norm': 36.08950424194336, 'learning_rate': 1.861751152073733e-05, 'epoch': 0.21}
{'loss': 224.6373, 'grad_norm': 31.241748809814453, 'learning_rate': 1.8463901689708145e-05, 'epoch': 0.23}
{'loss': 222.6778, 'grad_norm': 28.11034393310547, 'learning_rate': 1.8310291858678958e-05, 'epoch': 0.25}
{'loss': 221.1358, 'grad_norm': 27.218414306640625, 'learning_rate': 1.815668202764977e-05, 'epoch': 0.28}
{'loss': 220.4262, 'grad_norm': 28.530540466308594, 'learning_rate': 1.8003072196620586e-05, 'epoch': 0.3}
{'loss': 219.137, 'grad_norm': 30.71046257019043, 'learning_rate': 1.78494623655914e-05, 'epoch': 0.32}
{'loss': 217.5719, 'grad_norm': 31.093244552612305, 'learning_rate': 1.7695852534562215e-05, 'epoch': 0.34}
{'loss': 216.2177, 'grad_norm': 26.047964096069336, 'learning_rate': 1.7542242703533028e-05, 'epoch': 0.37}
{'loss': 213.4312, 'grad_norm': 24.9373722076416, 'learning_rate': 1.738863287250384e-05, 'epoch': 0.39}
{'loss': 215.5139, 'grad_norm': 29.272573471069336, 'learning_rate': 1.7235023041474656e-05, 'epoch': 0.41}
{'loss': 216.5779, 'grad_norm': 23.565095901489258, 'learning_rate': 1.708141321044547e-05, 'epoch': 0.44}
{'loss': 213.2752, 'grad_norm': 26.882055282592773, 'learning_rate': 1.6927803379416285e-05, 'epoch': 0.46}
{'loss': 211.8233, 'grad_norm': 21.33347511291504, 'learning_rate': 1.6774193548387098e-05, 'epoch': 0.48}
{'loss': 211.6636, 'grad_norm': 25.106002807617188, 'learning_rate': 1.6620583717357914e-05, 'epoch': 0.51}
{'loss': 211.4222, 'grad_norm': 29.94859504699707, 'learning_rate': 1.6466973886328726e-05, 'epoch': 0.53}
{'loss': 208.4997, 'grad_norm': 17.99738883972168, 'learning_rate': 1.6313364055299542e-05, 'epoch': 0.55}
{'loss': 209.1991, 'grad_norm': 20.90739631652832, 'learning_rate': 1.6159754224270355e-05, 'epoch': 0.57}
{'loss': 210.5577, 'grad_norm': 18.615236282348633, 'learning_rate': 1.600614439324117e-05, 'epoch': 0.6}
{'loss': 211.4861, 'grad_norm': 18.17849349975586, 'learning_rate': 1.5852534562211984e-05, 'epoch': 0.62}
{'loss': 210.2291, 'grad_norm': 27.049137115478516, 'learning_rate': 1.5698924731182796e-05, 'epoch': 0.64}
{'loss': 207.4393, 'grad_norm': 22.842126846313477, 'learning_rate': 1.5545314900153612e-05, 'epoch': 0.67}
{'loss': 207.4624, 'grad_norm': 22.105365753173828, 'learning_rate': 1.5391705069124425e-05, 'epoch': 0.69}
{'loss': 210.216, 'grad_norm': 24.73595428466797, 'learning_rate': 1.523809523809524e-05, 'epoch': 0.71}
{'loss': 208.3655, 'grad_norm': 16.929454803466797, 'learning_rate': 1.5084485407066054e-05, 'epoch': 0.74}
{'loss': 209.7267, 'grad_norm': 18.865863800048828, 'learning_rate': 1.4930875576036868e-05, 'epoch': 0.76}
{'loss': 207.5252, 'grad_norm': 18.759628295898438, 'learning_rate': 1.477726574500768e-05, 'epoch': 0.78}
{'loss': 206.9671, 'grad_norm': 20.55591583251953, 'learning_rate': 1.4623655913978497e-05, 'epoch': 0.8}
{'loss': 206.4886, 'grad_norm': 17.867475509643555, 'learning_rate': 1.447004608294931e-05, 'epoch': 0.83}
{'loss': 206.3487, 'grad_norm': 21.16921043395996, 'learning_rate': 1.4316436251920125e-05, 'epoch': 0.85}
{'loss': 206.1126, 'grad_norm': 20.358510971069336, 'learning_rate': 1.4162826420890938e-05, 'epoch': 0.87}
{'loss': 208.2602, 'grad_norm': 23.23797607421875, 'learning_rate': 1.4009216589861752e-05, 'epoch': 0.9}
{'loss': 206.6794, 'grad_norm': 21.067834854125977, 'learning_rate': 1.3855606758832567e-05, 'epoch': 0.92}
{'loss': 206.6213, 'grad_norm': 19.913660049438477, 'learning_rate': 1.3701996927803381e-05, 'epoch': 0.94}
{'loss': 205.4802, 'grad_norm': 20.36809730529785, 'learning_rate': 1.3548387096774194e-05, 'epoch': 0.97}
{'loss': 206.8675, 'grad_norm': 19.33378791809082, 'learning_rate': 1.339477726574501e-05, 'epoch': 0.99}
{'loss': 188.0086, 'grad_norm': 21.98272705078125, 'learning_rate': 1.3241167434715822e-05, 'epoch': 1.01}
{'loss': 205.1348, 'grad_norm': 21.440353393554688, 'learning_rate': 1.3087557603686638e-05, 'epoch': 1.03}
{'loss': 204.7714, 'grad_norm': 20.344043731689453, 'learning_rate': 1.2933947772657451e-05, 'epoch': 1.06}
{'loss': 206.2558, 'grad_norm': 24.739585876464844, 'learning_rate': 1.2780337941628265e-05, 'epoch': 1.08}
{'loss': 207.5688, 'grad_norm': 19.172353744506836, 'learning_rate': 1.262672811059908e-05, 'epoch': 1.1}
{'loss': 205.3159, 'grad_norm': 22.96541976928711, 'learning_rate': 1.2473118279569894e-05, 'epoch': 1.12}
{'loss': 205.9491, 'grad_norm': 19.95641326904297, 'learning_rate': 1.2319508448540707e-05, 'epoch': 1.15}
{'loss': 205.103, 'grad_norm': 18.031898498535156, 'learning_rate': 1.2165898617511523e-05, 'epoch': 1.17}
{'loss': 204.488, 'grad_norm': 20.497278213500977, 'learning_rate': 1.2012288786482335e-05, 'epoch': 1.19}
{'loss': 207.5773, 'grad_norm': 32.95076370239258, 'learning_rate': 1.185867895545315e-05, 'epoch': 1.22}
{'loss': 205.95, 'grad_norm': 19.42290496826172, 'learning_rate': 1.1705069124423964e-05, 'epoch': 1.24}
{'loss': 204.4752, 'grad_norm': 16.366880416870117, 'learning_rate': 1.1551459293394778e-05, 'epoch': 1.26}
{'loss': 205.2773, 'grad_norm': 16.238927841186523, 'learning_rate': 1.1397849462365593e-05, 'epoch': 1.29}
{'loss': 204.1697, 'grad_norm': 19.205224990844727, 'learning_rate': 1.1244239631336407e-05, 'epoch': 1.31}
{'loss': 204.6484, 'grad_norm': 19.128711700439453, 'learning_rate': 1.109062980030722e-05, 'epoch': 1.33}
{'loss': 205.0514, 'grad_norm': 17.074031829833984, 'learning_rate': 1.0937019969278036e-05, 'epoch': 1.35}
{'loss': 204.2626, 'grad_norm': 18.72908592224121, 'learning_rate': 1.0783410138248848e-05, 'epoch': 1.38}
{'loss': 207.9726, 'grad_norm': 29.037752151489258, 'learning_rate': 1.0629800307219663e-05, 'epoch': 1.4}
{'loss': 205.1357, 'grad_norm': 19.264019012451172, 'learning_rate': 1.0476190476190477e-05, 'epoch': 1.42}
{'loss': 203.9093, 'grad_norm': 23.025575637817383, 'learning_rate': 1.0322580645161291e-05, 'epoch': 1.45}
{'loss': 203.9861, 'grad_norm': 19.197561264038086, 'learning_rate': 1.0168970814132104e-05, 'epoch': 1.47}
{'loss': 204.9725, 'grad_norm': 21.44049835205078, 'learning_rate': 1.001536098310292e-05, 'epoch': 1.49}
{'loss': 206.2444, 'grad_norm': 29.24017906188965, 'learning_rate': 9.861751152073733e-06, 'epoch': 1.51}
{'loss': 205.9735, 'grad_norm': 18.86347007751465, 'learning_rate': 9.708141321044547e-06, 'epoch': 1.54}
{'loss': 205.0047, 'grad_norm': 19.482942581176758, 'learning_rate': 9.554531490015361e-06, 'epoch': 1.56}
{'loss': 205.2547, 'grad_norm': 22.252323150634766, 'learning_rate': 9.400921658986176e-06, 'epoch': 1.58}
{'loss': 202.6713, 'grad_norm': 23.204458236694336, 'learning_rate': 9.24731182795699e-06, 'epoch': 1.61}
{'loss': 203.8742, 'grad_norm': 18.391874313354492, 'learning_rate': 9.093701996927804e-06, 'epoch': 1.63}
{'loss': 202.6615, 'grad_norm': 20.50990104675293, 'learning_rate': 8.940092165898619e-06, 'epoch': 1.65}
{'loss': 202.5171, 'grad_norm': 25.73984146118164, 'learning_rate': 8.786482334869433e-06, 'epoch': 1.68}
{'loss': 202.9943, 'grad_norm': 17.648813247680664, 'learning_rate': 8.632872503840246e-06, 'epoch': 1.7}
{'loss': 205.4928, 'grad_norm': 25.16086769104004, 'learning_rate': 8.47926267281106e-06, 'epoch': 1.72}
{'loss': 203.8401, 'grad_norm': 26.740570068359375, 'learning_rate': 8.325652841781874e-06, 'epoch': 1.74}
{'loss': 203.7291, 'grad_norm': 21.011274337768555, 'learning_rate': 8.172043010752689e-06, 'epoch': 1.77}
{'loss': 204.5725, 'grad_norm': 21.49965476989746, 'learning_rate': 8.018433179723503e-06, 'epoch': 1.79}
{'loss': 203.9177, 'grad_norm': 23.424047470092773, 'learning_rate': 7.864823348694317e-06, 'epoch': 1.81}
{'loss': 205.6938, 'grad_norm': 23.716529846191406, 'learning_rate': 7.711213517665132e-06, 'epoch': 1.84}
{'loss': 204.6857, 'grad_norm': 25.261192321777344, 'learning_rate': 7.557603686635945e-06, 'epoch': 1.86}
{'loss': 204.0719, 'grad_norm': 19.05340003967285, 'learning_rate': 7.403993855606759e-06, 'epoch': 1.88}
{'loss': 205.4155, 'grad_norm': 24.335956573486328, 'learning_rate': 7.250384024577574e-06, 'epoch': 1.91}
{'loss': 205.353, 'grad_norm': 29.022069931030273, 'learning_rate': 7.096774193548388e-06, 'epoch': 1.93}
{'loss': 205.7824, 'grad_norm': 24.79468536376953, 'learning_rate': 6.9431643625192015e-06, 'epoch': 1.95}
{'loss': 204.84, 'grad_norm': 22.61225700378418, 'learning_rate': 6.789554531490016e-06, 'epoch': 1.97}
{'loss': 204.8152, 'grad_norm': 22.95793342590332, 'learning_rate': 6.63594470046083e-06, 'epoch': 2.0}
{'loss': 184.8792, 'grad_norm': 25.575254440307617, 'learning_rate': 6.4823348694316445e-06, 'epoch': 2.02}
{'loss': 205.362, 'grad_norm': 33.766109466552734, 'learning_rate': 6.328725038402458e-06, 'epoch': 2.04}
{'loss': 207.0039, 'grad_norm': 22.68694496154785, 'learning_rate': 6.175115207373272e-06, 'epoch': 2.06}
{'loss': 205.0981, 'grad_norm': 21.727258682250977, 'learning_rate': 6.021505376344087e-06, 'epoch': 2.09}
{'loss': 205.2895, 'grad_norm': 19.974607467651367, 'learning_rate': 5.867895545314901e-06, 'epoch': 2.11}
{'loss': 205.2635, 'grad_norm': 19.912233352661133, 'learning_rate': 5.7142857142857145e-06, 'epoch': 2.13}
{'loss': 204.1479, 'grad_norm': 20.754016876220703, 'learning_rate': 5.560675883256529e-06, 'epoch': 2.16}
{'loss': 203.6603, 'grad_norm': 19.32797622680664, 'learning_rate': 5.407066052227343e-06, 'epoch': 2.18}
{'loss': 203.1611, 'grad_norm': 18.875585556030273, 'learning_rate': 5.253456221198157e-06, 'epoch': 2.2}
{'loss': 202.2141, 'grad_norm': 19.219730377197266, 'learning_rate': 5.099846390168971e-06, 'epoch': 2.23}
{'loss': 203.7534, 'grad_norm': 22.60515594482422, 'learning_rate': 4.946236559139785e-06, 'epoch': 2.25}
{'loss': 203.3382, 'grad_norm': 22.452695846557617, 'learning_rate': 4.7926267281106e-06, 'epoch': 2.27}
{'loss': 205.9637, 'grad_norm': 20.294937133789062, 'learning_rate': 4.639016897081414e-06, 'epoch': 2.29}
{'loss': 204.2725, 'grad_norm': 21.362163543701172, 'learning_rate': 4.4854070660522275e-06, 'epoch': 2.32}
{'loss': 200.4564, 'grad_norm': 20.554311752319336, 'learning_rate': 4.331797235023042e-06, 'epoch': 2.34}
{'loss': 203.614, 'grad_norm': 21.41278648376465, 'learning_rate': 4.178187403993856e-06, 'epoch': 2.36}
{'loss': 203.3064, 'grad_norm': 17.58690643310547, 'learning_rate': 4.0245775729646705e-06, 'epoch': 2.39}
{'loss': 203.6413, 'grad_norm': 24.462677001953125, 'learning_rate': 3.870967741935484e-06, 'epoch': 2.41}
{'loss': 203.4115, 'grad_norm': 21.34147071838379, 'learning_rate': 3.7173579109062983e-06, 'epoch': 2.43}
{'loss': 204.0729, 'grad_norm': 20.694480895996094, 'learning_rate': 3.5637480798771122e-06, 'epoch': 2.46}
{'loss': 204.8439, 'grad_norm': 23.793228149414062, 'learning_rate': 3.4101382488479266e-06, 'epoch': 2.48}
{'loss': 203.5615, 'grad_norm': 18.193445205688477, 'learning_rate': 3.2565284178187405e-06, 'epoch': 2.5}
{'loss': 203.6942, 'grad_norm': 24.02614402770996, 'learning_rate': 3.1029185867895553e-06, 'epoch': 2.52}
{'loss': 202.1022, 'grad_norm': 17.897939682006836, 'learning_rate': 2.9493087557603687e-06, 'epoch': 2.55}
{'loss': 203.0742, 'grad_norm': 28.104978561401367, 'learning_rate': 2.7956989247311827e-06, 'epoch': 2.57}
{'loss': 205.8851, 'grad_norm': 23.150453567504883, 'learning_rate': 2.642089093701997e-06, 'epoch': 2.59}
{'loss': 203.9058, 'grad_norm': 23.15672492980957, 'learning_rate': 2.4884792626728113e-06, 'epoch': 2.62}
{'loss': 203.4001, 'grad_norm': 21.977195739746094, 'learning_rate': 2.3348694316436257e-06, 'epoch': 2.64}
{'loss': 202.9982, 'grad_norm': 21.128446578979492, 'learning_rate': 2.1812596006144396e-06, 'epoch': 2.66}
{'loss': 204.0271, 'grad_norm': 24.268007278442383, 'learning_rate': 2.027649769585254e-06, 'epoch': 2.68}
{'loss': 202.8425, 'grad_norm': 25.80496597290039, 'learning_rate': 1.8740399385560678e-06, 'epoch': 2.71}
{'loss': 203.177, 'grad_norm': 21.367219924926758, 'learning_rate': 1.720430107526882e-06, 'epoch': 2.73}
{'loss': 201.8437, 'grad_norm': 19.856061935424805, 'learning_rate': 1.5668202764976959e-06, 'epoch': 2.75}
{'loss': 204.0383, 'grad_norm': 20.30842399597168, 'learning_rate': 1.41321044546851e-06, 'epoch': 2.78}
{'loss': 203.4799, 'grad_norm': 27.39141082763672, 'learning_rate': 1.259600614439324e-06, 'epoch': 2.8}
{'loss': 203.4343, 'grad_norm': 24.899240493774414, 'learning_rate': 1.1059907834101384e-06, 'epoch': 2.82}
{'loss': 205.9661, 'grad_norm': 21.598886489868164, 'learning_rate': 9.523809523809525e-07, 'epoch': 2.85}
{'loss': 203.3912, 'grad_norm': 19.44171714782715, 'learning_rate': 7.987711213517666e-07, 'epoch': 2.87}
{'loss': 204.5317, 'grad_norm': 21.054702758789062, 'learning_rate': 6.451612903225807e-07, 'epoch': 2.89}
{'loss': 204.6524, 'grad_norm': 20.595294952392578, 'learning_rate': 4.915514592933948e-07, 'epoch': 2.91}
{'loss': 202.482, 'grad_norm': 22.153291702270508, 'learning_rate': 3.3794162826420895e-07, 'epoch': 2.94}
{'loss': 203.8161, 'grad_norm': 28.083932876586914, 'learning_rate': 1.8433179723502305e-07, 'epoch': 2.96}
{'loss': 203.5573, 'grad_norm': 26.82177734375, 'learning_rate': 3.0721966205837177e-08, 'epoch': 2.98}
{'train_runtime': 6584.9737, 'train_samples_per_second': 3.171, 'train_steps_per_second': 0.099, 'train_loss': 207.84757177584365, 'epoch': 2.99}
